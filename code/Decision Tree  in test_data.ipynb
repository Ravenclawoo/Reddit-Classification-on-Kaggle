{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/user/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "60000\n",
      "100000\n",
      "Prediction is Done!!\n",
      "The running time is: 96.82750725746155\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from six.moves import zip\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#input: raw_train\n",
    "#output: X and y\n",
    "# document = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
    "# document = document[['Document', 'Category']]\n",
    "# print(document)\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#some tools and list to use in the future\n",
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "# print(english_words)\n",
    "\n",
    "\n",
    "\n",
    "#remove special symbols and raw preprocess\n",
    "def remove_abnormal(text):\n",
    "\t#remove whitespaces\n",
    "\ttext = text.strip()\n",
    "\t#lower case\n",
    "\ttext = text.lower()\n",
    "\t#remove numbers\n",
    "\ttext = re.sub('\\d+', '', text)\n",
    "\ttext = re.sub('_', '', text)\n",
    "\ttext = re.sub('\\s+', ' ', text)\n",
    "\t#remove html links\n",
    "\ttext = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n",
    "\t#replace special characters with ''\n",
    "\ttext = re.sub('[^\\w\\s]', '', text)\n",
    "\treturn text\n",
    "\n",
    "#remove stop words\n",
    "def remove_stopwords(text):\n",
    "\ttokens = tokenizer.tokenize(text)\n",
    "\ttokens = [token.strip() for token in tokens]\n",
    "\ttext = [token for token in tokens if token.lower() not in stop_words]\n",
    "\treturn \" \".join(text)\n",
    "\t\n",
    "\n",
    "#remove non-english words\n",
    "def remove_nonenglish(text):\n",
    "\ttokens = tokenizer.tokenize(text)\n",
    "\ttokens = [token.strip() for token in tokens]\n",
    "\ttext = [token for token in tokens if token.lower() in english_words]\n",
    "\treturn \" \".join(text)\n",
    "\n",
    "\n",
    "#lemmatization\n",
    "def words_lemmatization(text):\n",
    "\ttokens = tokenizer.tokenize(text)\n",
    "\ttokens = [lemmatizer.lemmatize(token.strip()) for token in tokens]\n",
    "\ttext = [token for token in tokens]\n",
    "\treturn \" \".join(text)\n",
    "\n",
    "\n",
    "def data_preprocess(corpus):\n",
    "\tpreprocessed = []\n",
    "\tfor sentence in corpus:\n",
    "\t\t# remove special character and normalize docs\n",
    "\t\tsentence = remove_abnormal(sentence)\n",
    "\t\t# remove stopwords \n",
    "\t\tsentence = remove_stopwords(sentence)\n",
    "\t\t# remove non-english\n",
    "\t\t# sentence = remove_nonenglish(sentence)\n",
    "\t\t# words lemmatize\n",
    "\t\tsentence = words_lemmatization(sentence)\n",
    "\t\tpreprocessed.append(sentence)\t \n",
    "\treturn preprocessed\n",
    "\n",
    "\n",
    "\n",
    "# path = os.path.abspath(os.path.dirname(sys.argv[0]))\n",
    "\n",
    "\n",
    "#read reddit_train.csv\n",
    "comments = pd.read_csv('reddit_train.csv', usecols=[1], encoding='Latin-1')\n",
    "comments = np.array(comments)\n",
    "\n",
    "corpus = []\n",
    "for i in range(0, len(comments)):\n",
    "\tcorpus.extend(comments[i].astype(str))\n",
    "\n",
    "\n",
    "#read all-y-numbers.csv\n",
    "subreddits = pd.read_csv('all-y-numbers.csv', usecols=[1])\n",
    "subreddits = np.array(subreddits)\n",
    "\n",
    "labels = []\n",
    "for i in range(0, len(subreddits)):\n",
    "\tlabels.extend(subreddits[i].astype(str))           # training labels\n",
    "labels = np.array(labels)\n",
    "\n",
    "#read reddit_test.csv\n",
    "test_comments = pd.read_csv('reddit_test.csv',usecols=[1], encoding='Latin-1')\n",
    "test_comments = np.array(test_comments)\n",
    "\n",
    "test_corpus = []\n",
    "for i in range(0, len(test_comments)):\n",
    "\ttest_corpus.extend(test_comments[i].astype(str))\n",
    "\n",
    "\n",
    "train_x = data_preprocess(corpus)\n",
    "test_x = data_preprocess(test_corpus)\n",
    "\n",
    "\n",
    "preprocessed_corpus = train_x + test_x\n",
    "\n",
    "\n",
    "\n",
    "print(len(preprocessed_corpus))\n",
    "\n",
    "\n",
    "\n",
    "c_vector = TfidfVectorizer(max_features=60000, binary=False)\n",
    "# c_vector = TfidfVectorizer(max_features=1, binary=False)\n",
    "c_vector.fit_transform(preprocessed_corpus)\n",
    "words_bag = c_vector.get_feature_names()\n",
    "print(len(words_bag))\n",
    "print(len(preprocessed_corpus))\n",
    "feature_matrix_train = c_vector.transform(train_x)     # training features\n",
    "feature_matrix_test = c_vector.transform(test_x)       # testing features\n",
    "\n",
    "#print(type(feature_matrix_train))\n",
    "#print(type(feature_matrix_test))\n",
    "# print(labels)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "###########################################################################\n",
    "###########################################################################\n",
    "\n",
    "#produce three kinds of dataï¼šfeature_matrix_train, labels, feature_matrix_test\n",
    "# sfit(feature_matrix_train, labels),  and then  predict(feature_matrix_test)\n",
    "\n",
    "\n",
    "# five-fold cross validation\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "# for train_index, validation_index in skf.split(feature_matrix_train, labels):\n",
    "#     features_train, features_validation = feature_matrix_train[train_index], feature_matrix_train[validation_index]\n",
    "#     labels_train,labels_validation = labels[train_index],labels[validation_index]\n",
    "#     print(\"Training sample class distribution: \", features_train.shape[0])\n",
    "#     print(\"Validation sample class distribution: \", features_validation.shape[0])\n",
    "#     # Decision Tree classifier\n",
    "#     clf=DecisionTreeClassifier(splitter='random')\n",
    "#     clf.fit(features_train, labels_train)\n",
    "#     print(\"Training accuracy is:\", clf.score(features_train, labels_train))\n",
    "#     print(\"Training is finished, ready to perform validation.\")\n",
    "#     labels_validation_predict = clf.predict(features_validation)\n",
    "#     print(\"Validation accuracy is: \", clf.score(features_validation, labels_validation))\n",
    "\n",
    "clf=DecisionTreeClassifier(splitter='random')\n",
    "clf.fit(feature_matrix_train,labels)\n",
    "\n",
    "prediction = clf.predict(feature_matrix_test)\n",
    "print(\"Prediction is Done!!\")\n",
    "\n",
    "\n",
    "# compute the running time\n",
    "end = time.time()\n",
    "print(\"The running time is:\", (end-start))\n",
    "\n",
    "prediction = prediction.tolist()\n",
    "\n",
    "\n",
    "for i in range(0, 30000):\n",
    "    if prediction[i] == '0':\n",
    "        prediction[i] = 'hockey'\n",
    "    if prediction[i] == '1':\n",
    "        prediction[i] = 'nba'\n",
    "    if prediction[i] == '2':\n",
    "        prediction[i] = 'leagueoflegends'\n",
    "    if prediction[i] == '3':\n",
    "        prediction[i] = 'soccer'\n",
    "    if prediction[i] == '4':\n",
    "        prediction[i] = 'funny'\n",
    "    if prediction[i] == '5':\n",
    "        prediction[i] = 'movies'\n",
    "    if prediction[i] == '6':\n",
    "        prediction[i] = 'anime'\n",
    "    if prediction[i] == '7':\n",
    "        prediction[i] = 'Overwatch'\n",
    "    if prediction[i] == '8':\n",
    "        prediction[i] = 'trees'\n",
    "    if prediction[i] == '9':\n",
    "        prediction[i] = 'GlobalOffensive'\n",
    "    if prediction[i] == '10':\n",
    "        prediction[i] = 'nfl'\n",
    "    if prediction[i] == '11':\n",
    "        prediction[i] = 'AskReddit'\n",
    "    if prediction[i] == '12':\n",
    "        prediction[i] = 'gameofthrones'\n",
    "    if prediction[i] == '13':\n",
    "        prediction[i] = 'conspiracy'\n",
    "    if prediction[i] == '14':\n",
    "        prediction[i] = 'worldnews'\n",
    "    if prediction[i] == '15':\n",
    "        prediction[i] = 'wow'\n",
    "    if prediction[i] == '16':\n",
    "        prediction[i] = 'europe'\n",
    "    if prediction[i] == '17':\n",
    "        prediction[i] = 'canada'\n",
    "    if prediction[i] == '18':\n",
    "        prediction[i] = 'Music'\n",
    "    if prediction[i] == '19':\n",
    "        prediction[i] = 'baseball'\n",
    "# write testing result into csv. file\n",
    "prediction = pd.DataFrame(prediction)\n",
    "prediction.to_csv('DT-predict-tfidf-60000.csv', index=True, header=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
