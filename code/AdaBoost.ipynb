{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/user/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "40000\n",
      "100000\n",
      "Training sample class distribution:  56000\n",
      "Validation sample class distribution:  14000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk import FreqDist\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from six.moves import zip\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# input: raw_train\n",
    "# output: X and y\n",
    "# document = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
    "# document = document[['Document', 'Category']]\n",
    "# print(document)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# some tools and list to use in the future\n",
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "# print(english_words)\n",
    "\n",
    "\n",
    "# remove special symbols and raw preprocess\n",
    "def remove_abnormal(text):\n",
    "    # remove whitespaces\n",
    "    text = text.strip()\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    # remove numbers\n",
    "    text = re.sub('\\d+', '', text)\n",
    "    text = re.sub('_', '', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    # remove html links\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    # replace special characters with ''\n",
    "    text = re.sub('[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# remove stop words\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    text = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "# remove non-english words\n",
    "def remove_nonenglish(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    text = [token for token in tokens if token.lower() in english_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "# lemmatization\n",
    "def words_lemmatization(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(token.strip()) for token in tokens]\n",
    "    text = [token for token in tokens]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def data_preprocess(corpus):\n",
    "    preprocessed = []\n",
    "    for sentence in corpus:\n",
    "        # remove special character and normalize docs\n",
    "        sentence = remove_abnormal(sentence)\n",
    "        # remove stopwords\n",
    "        sentence = remove_stopwords(sentence)\n",
    "        # remove non-english\n",
    "        # sentence = remove_nonenglish(sentence)\n",
    "        # words lemmatize\n",
    "        sentence = words_lemmatization(sentence)\n",
    "        preprocessed.append(sentence)\n",
    "    return preprocessed\n",
    "\n",
    "\n",
    "# path = os.path.abspath(os.path.dirname(sys.argv[0]))\n",
    "\n",
    "# import reddit_train.csv\n",
    "comments = pd.read_csv('reddit_train.csv', usecols=[1], encoding='Latin-1')\n",
    "comments = np.array(comments)\n",
    "\n",
    "corpus = []\n",
    "for i in range(0, len(comments)):\n",
    "    corpus.extend(comments[i].astype(str))\n",
    "\n",
    "# import all-y-numbers.csv\n",
    "subreddits = pd.read_csv( 'all-y-numbers.csv', usecols=[1])\n",
    "subreddits = np.array(subreddits)\n",
    "\n",
    "labels = []\n",
    "for i in range(0, len(subreddits)):\n",
    "    labels.extend(subreddits[i].astype(str))  # training labels\n",
    "labels = np.array(labels)\n",
    "\n",
    "# import reddit_test.csv\n",
    "test_comments = pd.read_csv('reddit_test.csv', usecols=[1], encoding='Latin-1')\n",
    "test_comments = np.array(test_comments)\n",
    "\n",
    "test_corpus = []\n",
    "for i in range(0, len(test_comments)):\n",
    "    test_corpus.extend(test_comments[i].astype(str))\n",
    "\n",
    "train_x = data_preprocess(corpus)\n",
    "test_x = data_preprocess(test_corpus)\n",
    "\n",
    "preprocessed_corpus = train_x + test_x\n",
    "\n",
    "print(len(preprocessed_corpus))\n",
    "\n",
    "c_vector = TfidfVectorizer(max_features=40000, binary=False)\n",
    "c_vector.fit_transform(preprocessed_corpus)\n",
    "words_bag = c_vector.get_feature_names()\n",
    "print(len(words_bag))\n",
    "print(len(preprocessed_corpus))\n",
    "feature_matrix_train = c_vector.transform(train_x)  # training features\n",
    "feature_matrix_test = c_vector.transform(test_x)  # testing features\n",
    "\n",
    "# print(type(feature_matrix_train))\n",
    "# print(type(feature_matrix_test))\n",
    "# print(labels)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "###########################################################################\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "# five-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, validation_index in skf.split(feature_matrix_train, labels):\n",
    "    features_train, features_validation = feature_matrix_train[train_index], feature_matrix_train[validation_index]\n",
    "    labels_train, labels_validation = labels[train_index], labels[validation_index]\n",
    "    print(\"Training sample class distribution: \", features_train.shape[0])\n",
    "    print(\"Validation sample class distribution: \", features_validation.shape[0])\n",
    "    # AdaBoost classifier\n",
    "    bdt = AdaBoostClassifier(DecisionTreeClassifier(min_samples_split=20, min_samples_leaf=5),\n",
    "                             algorithm=\"SAMME\", n_estimators=250, learning_rate=0.1)\n",
    "    bdt.fit(features_train, labels_train)\n",
    "    print(\"Training accuracy is:\", bdt.score(features_train, labels_train))\n",
    "    print(\"Training is finished, ready to perform validation.\")\n",
    "    labels_predict = bdt.predict(features_validation)\n",
    "    print(\"Validation accuracy is: \", (bdt.score(features_validation, labels_validation)))\n",
    "\n",
    "\n",
    "# compute the running time\n",
    "end = time.time()\n",
    "print(\"The running time is:\", (end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
